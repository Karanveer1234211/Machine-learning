#!/usr/bin/env python3
"""Build a single training dataframe for XGBoost.

The script scans your *_daily.parquet cache files, extracts all columns whose
name starts with "d" (daily indicator features) and the forward-looking
predictability targets generated by ``add return.py``.  All rows with missing
future targets are dropped so the resulting dataframe is directly consumable by
an XGBoost trainer.
"""
from __future__ import annotations

import argparse
import glob
import os
from typing import Iterable, List

import polars as pl

FUTURE_TARGET_COLUMNS: tuple[str, ...] = (
    "ret_next1d_pct",
    "ret_next3d_pct",
    "ret_next1w_pct",
    "peak_ret_next3d_pct",
    "peak_ret_next5d_pct",
    "max_dd_next3d_pct",
    "max_dd_next5d_pct",
)

DEFAULT_ROOT = r"C:\\Users\\karanvsi\\Desktop\\Pycharm\\Cache\\cache_daily_new"
PATTERN = "*_daily.parquet"
DEFAULT_OUTPUT = "xgboost_daily.parquet"


def safe_read_parquet(path: str) -> pl.DataFrame:
    """Read a parquet with Polars, falling back to pandas if needed."""
    try:
        return pl.read_parquet(path)
    except Exception as exc:
        import pandas as pd

        try:
            pdf = pd.read_parquet(path, engine="pyarrow")
        except Exception as pandas_exc:
            raise RuntimeError(f"read_parquet failed for {path}: {pandas_exc}") from exc

        if "timestamp" in pdf.columns:
            ts = pd.to_datetime(pdf["timestamp"], errors="coerce", utc=True)
            try:
                ts = ts.dt.tz_localize(None)
            except Exception:
                pass
            pdf["timestamp"] = ts

        return pl.from_pandas(pdf)


def extract_features(df: pl.DataFrame, *, symbol: str | None) -> pl.DataFrame:
    indicator_cols = [col for col in df.columns if col.startswith("d")]
    if not indicator_cols:
        raise ValueError("No indicator columns starting with 'd' were found")

    available_targets = [col for col in FUTURE_TARGET_COLUMNS if col in df.columns]
    if not available_targets:
        raise ValueError("No future predictability columns present. Run 'add return.py' first.")

    keep_cols: List[str] = indicator_cols + available_targets
    feature_df = df.select(keep_cols)
    feature_df = feature_df.drop_nulls(subset=available_targets)

    if symbol is not None:
        feature_df = feature_df.with_columns(pl.lit(symbol).alias("symbol"))

    return feature_df


def concat_frames(frames: Iterable[pl.DataFrame]) -> pl.DataFrame:
    frames_list = list(frames)
    if not frames_list:
        raise ValueError("No dataframes to concatenate")
    return pl.concat(frames_list, how="diagonal_relaxed")


def parse_symbol(path: str) -> str:
    base = os.path.basename(path)
    if base.endswith("_daily.parquet"):
        base = base[: -len("_daily.parquet")]
    return base


def main() -> None:
    parser = argparse.ArgumentParser(description="Create a single XGBoost dataframe from daily cache files.")
    parser.add_argument("--root", default=DEFAULT_ROOT, help="Folder that contains *_daily.parquet files")
    parser.add_argument("--pattern", default=PATTERN, help="Glob pattern to locate parquet files")
    parser.add_argument("--output", default=DEFAULT_OUTPUT, help="Output parquet path for the merged dataframe")
    parser.add_argument("--limit", type=int, default=None, help="Optional limit on number of files to process")
    args = parser.parse_args()

    if not os.path.isdir(args.root):
        raise SystemExit(f"Folder not found: {args.root}")

    pattern = os.path.join(args.root, args.pattern)
    files = sorted(glob.glob(pattern))
    if args.limit is not None:
        files = files[: args.limit]

    if not files:
        raise SystemExit("No matching parquet files.")

    frames: List[pl.DataFrame] = []
    for path in files:
        print(f"Loading {path}")
        df = safe_read_parquet(path)
        symbol = parse_symbol(path)
        feature_df = extract_features(df, symbol=symbol)
        frames.append(feature_df)

    merged = concat_frames(frames)
    print(f"Writing {args.output} with {merged.height} rows and {merged.width} columns")
    merged.write_parquet(args.output, compression="snappy")


if __name__ == "__main__":
    main()
